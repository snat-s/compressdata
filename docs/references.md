# Paper References

TRACE: A Fast Transformer-based General-Purpose Lossless Compressor: https://arxiv.org/pdf/2203.16114.pdf
Faster and Stronger Lossless Compression with Optimized Autoregressive Framework: https://kanonjz.github.io/academic/share/pac-dac23.pdf
Accelerating General-purpose Lossless Compression via Simple and Scalable Parameterization: https://dl.acm.org/doi/abs/10.1145/3503161.3548413
RWKV: https://arxiv.org/pdf/2305.13048.pdf
Mamba: https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf
NNCP: https://bellard.org/nncp/nncp.pdf
S4: https://arxiv.org/pdf/2111.00396.pdf
OREO: https://dl.acm.org/doi/abs/10.1145/3503161.3548413
DZIP: https://arxiv.org/abs/1911.03572
Osoa: One-Shot Online Adaptation of Deep Generative Models for Lossless Compression: https://arxiv.org/pdf/2111.01662.pdf
Attention is all you need: https://arxiv.org/abs/1706.03762
Long short-term memory: https://www.bioinf.jku.at/publications/older/2604.pdf
LOSSLESS DATA COMPRESSION WITH TRANSFORMER: https://openreview.net/attachment?id=Hygi7xStvS&name=original_pdf
Handbook of Data Compression: https://link.springer.com/book/10.1007/978-1-84882-903-9 
Compressive Transformers for Long-Range Sequence Modelling: https://arxiv.org/abs/1911.05507

# Datasets

LTCB (enwik8 & enwik9): https://www.mattmahoney.net/dc/text.html
Silesia Corpus: https://sun.aei.polsl.pl//~sdeor/index.php?page=silesia
PG19: https://github.com/google-deepmind/pg19?tab=readme-ov-file

# Code

A Fast Transformer-based General-Purpose Lossless Compressor: https://github.com/mynotwo/A-Fast-Transformer-based-General-Purpose-LosslessCompressor
NanoRWKV: https://github.com/BlinkDL/nanoRWKV
Mamba: https://github.com/state-spaces/mamba/tree/main
S4: https://github.com/state-spaces/s4
CMIX: https://github.com/byronknoll/cmix

